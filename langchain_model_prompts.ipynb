{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Langchain : Models -> LLMs </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "result = llm.invoke('What is the capital of india')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Models -> ChatModels (Closed Source)</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# temperature is used to adjust the creativity of the model\n",
    "# max_completion_tokens is used to limit the response, tokens = words\n",
    "model=ChatOpenAI(model=\"gpt-4\",temperature=0.6,max_completion_tokens=10)\n",
    "\n",
    "result = model.invoke(\"Hello, how are you?\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claude\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model=ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "#Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "result = model.invoke(\"What is the capital of india\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Models -> ChatModels (Open Source) </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India's capital city is New Delhi, also known as the Republic Day Capital, which continues to host the President of India. The capital of India is currently New Delhi, and the city is the site of Delhi's Parliament, the Supreme Court, and several government ministries and organizations. The state capital of Delhi, where the state administration and several important ministries like the Union Finance Department and Planning Commission of India are situated, is called Delhi. If desired, one can change the capital if they want to simply change the capital city where their home state is located.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm=HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"What is the capital of India?\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Models -> Embedding Model (Closed Source)</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Query\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=32)\n",
    "\n",
    "result = embeddings.embed_query(\"Delhi is the Capital of India\")\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Docs Embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=32)\n",
    "\n",
    "documents=[\n",
    "    \"Delhi is the Capital of India\",\n",
    "    \"India is a great country\"\n",
    "    ]\n",
    "\n",
    "result = embeddings.embed_documents(documents)\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Models -> Embedding Model (Open Source)</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Query\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "text=\"Delhi is the Capital of the India\"\n",
    "\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Query\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "documents=[\n",
    "    \"Delhi is the Capital of India\",\n",
    "    \"India is a great country\"\n",
    "    ]\n",
    "\n",
    "vector = embeddings.embed_documents(documents)\n",
    "\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tell me about NASA\n",
      "Most similar document: NASA's Mars rovers collect soil samples to study the planet's geology and atmosphere.\n",
      "Similarity score: 0.4187291827712638\n"
     ]
    }
   ],
   "source": [
    "# document similarity\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=300)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = [\n",
    "    \"Quantum computing has the potential to revolutionize data encryption and problem-solving.\",  # Technology\n",
    "    \"Medical imaging powered by AI helps doctors detect diseases at an early stage.\",  # Healthcare\n",
    "    \"Stock market predictions rely on statistical models and historical data analysis.\",  # Finance\n",
    "    \"NASA's Mars rovers collect soil samples to study the planet's geology and atmosphere.\",  # Space Science\n",
    "    \"Football requires teamwork, strategy, and endurance to succeed at a competitive level.\"  # Sports\n",
    "]\n",
    "\n",
    "query=\"tell me about NASA\"\n",
    "doc_embeddings = embeddings.embed_documents(documents)\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "scores = cosine_similarity([query_embedding],doc_embeddings)[0]\n",
    "\n",
    "index , score = sorted(list(enumerate(scores)),key=lambda x:x[1])[-1]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Most similar document: {documents[index]}\")\n",
    "print(f\"Similarity score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Prompts</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The population of India is estimated to be around **1.4 billion** people.\n"
     ]
    }
   ],
   "source": [
    "# Static Prompts\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "res = model.invoke(\"What is the Population of India\")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BMW 3 Series is generally considered the best-selling model in BMW's history.\n"
     ]
    }
   ],
   "source": [
    "#Dynamic Prompts\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"Brand\"],\n",
    "    template=\"\"\"\n",
    "    Which are the Best Selling Model of {brand} in 1 line\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=prompt)\n",
    "\n",
    "res = chain.run({\"brand\":\"BMW\"})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the school clock get detention?\n",
      "\n",
      "It kept tocking off.\n"
     ]
    }
   ],
   "source": [
    "#ChatPrompt Template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\",temperature=0.95)\n",
    "\n",
    "prompt  = ChatPromptTemplate.from_messages([\n",
    "    ('system',\"You are a helpful Assistant\"),\n",
    "    ('user',\"Tell me a joke on {topic}\")\n",
    "    \n",
    "]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=prompt)\n",
    "\n",
    "res=chain.invoke({\"topic\":\"School\"})\n",
    "\n",
    "print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Result : Food : Broccoli\n",
      "Type : Veggies\n"
     ]
    }
   ],
   "source": [
    "# FewShotPromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate , PromptTemplate\n",
    "\n",
    "examples=[\n",
    "    {\"Food\":\"Banana\",\"Type\":\"Fruit\"},\n",
    "    {\"Food\":\"Potato\",\"Type\":\"Veggies\"},\n",
    "    {\"Food\":\"Apple\",\"Type\":\"Fruit\"}\n",
    "]\n",
    "\n",
    "example_prompt=PromptTemplate(\n",
    "    input_variables=['Food','Type'],\n",
    "    template = \"Food : {Food} \\nType : {Type}\\n\"\n",
    ")\n",
    "\n",
    "few_shot_prompt=FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Classify the Type of the Following Food Products\",\n",
    "    suffix=\"Food : {Food}\\n Type\",\n",
    "    input_variables=['Food']\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "chain = LLMChain(llm=llm , prompt=few_shot_prompt)\n",
    "\n",
    "input_food =\"Broccoli\" \n",
    "\n",
    "response = chain.run(\n",
    "    {\"Food\":input_food}\n",
    ")\n",
    "\n",
    "print(f\"Classification Result : {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI :  Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Messages (static)\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Who won the 2022 World Cup?\"),\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User : \")\n",
    "    if user_input == \"Exit\":\n",
    "        break\n",
    "    messages.append(HumanMessage(user_input))\n",
    "\n",
    "    response = llm(messages)\n",
    "    messages.append(AIMessage(response.content))\n",
    "\n",
    "    print(\"AI : \", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`pip` is a package installer for Python. It helps you easily install, update, and manage external libraries and dependencies your Python projects need from the Python Package Index (PyPI).\n"
     ]
    }
   ],
   "source": [
    "# Messages (Dynamic)\n",
    "from langchain_core.prompts import ChatPromptTemplate ,SystemMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system','You are a helpful {domain} expert'),\n",
    "    ('human','Explain in Few Terms what is {topic}')\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=llm , prompt=chat_template)\n",
    "\n",
    "res = chain.run({\n",
    "    'domain': 'Python',\n",
    "    'topic': 'pip'\n",
    "})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful assistant.\n",
      "Human: Hi there!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hi! How can I help you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful assistant.\n",
      "Human: Hi there!\n",
      "AI: Hi! How can I help you today?\n",
      "Human: What did I just say?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You just said \"Hi there!\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Setup LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# 2. Setup memory to store conversation\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 3. Build the prompt with MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # This will dynamically insert chat history\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# 4. Create the chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")                                                                                                                       \n",
    "\n",
    "# 5. Run conversation\n",
    "print(chain.run(input=\"Hi there!\"))\n",
    "print(chain.run(input=\"What did I just say?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\" , temperature=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
